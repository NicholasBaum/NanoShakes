# -*- coding: utf-8 -*-
"""SimpleShakes.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1M1PIBKAd78EKb9r_SLmQ02n7RCSHY_K5
"""

# get data, the exclamation mark tells python to run the command by your operating system not by the runtime/shell
!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt

import torch
import torch.nn as nn
from torch.nn import functional as F

# hyper parameters
input_size = 8
batch_size = 4
dropout = 0

eval_interval = 300
learning_rate = 1e-2
epoch_count = 3000
eval_average_loss_n = 10
device = 'cuda' if torch.cuda.is_available() else 'cpu'
torch.manual_seed(1337)

# load data
with open('input.txt', 'r', encoding='utf-8') as f:
  text = f.read()

vocab = sorted(list(set(text)))
vocab_size = len(vocab)

# encoding
char2ind = {ch:i for i,ch in enumerate(vocab)}
ind2char = {i:ch for i,ch in enumerate(vocab)}

encode = lambda x: [char2ind[c] for c in x]
decode = lambda x: ''.join(ind2char[i] for i in x)

# training data
data = torch.tensor(encode(text), dtype=torch.long)
n = int(0.9*len(data))
train_data = data[:n]
val_data = data[n:]

def getBatch(split):
  data = train_data if split=='train' else val_data
  """
   second arg is a tuple indicating the size of the random integers array
   the trailing comma is needed otherwise the brackets are without meaning
   it would be just an integer wrapped in brackets
  """
  # create random starting points
  ind = torch.randint(len(data)-input_size, (batch_size,)) 
  x = torch.stack([data[i:i+input_size] for i in ind])
  # y is x shifted one position to the right, so y[i] is the next token and the correct guess for x[i] in a sentence
  y = torch.stack([data[i+1:i+input_size+1] for i in ind])
  x, y = x.to(device), y.to(device)
  return x,y

# simple model
class SimpleModel(nn.Module):
  def __init__(self, vocab_size):
    super().__init__()
    # i think this is nothing more than a look up table
    self.embedding = nn.Embedding(vocab_size, vocab_size)

  def forward(self, x, targets = None):
    # x and targets are BxT e.g. 32 (batch_size) x 8 (sentence_length)
    # returns a BxTxC tensor where C is the vocab_size dimension    
    
    # for understanding embedding([33]) would return a vector of length vocab_size
    # indcating what token has the highest probability of coming after token 33
    # you can also pass a vector in than the result is a matrix one probability row per token
    # and if you pass a matrix in the result is a 3d tensor
    
    # remark: one sentence of length 8 gives you 8 tokens to predict    
    output = self.embedding(x)

    if targets == None:
      loss = None
    else:
      # have to reshape data for cross_entropy
      # i think other than with the embedding the tokens have to be in a list now
      B,T,C = output.shape
      output = output.view(B*T,C)      
      targets = targets.view(B*T)
      loss = F.cross_entropy(output, targets)

    return output, loss

  def generate(self, x, count):
    for _ in range(count):
      out, loss = self(x)
      # only select last tokens
      out = out[:,-1,:] #BxC
      # converting to probabilties 
      # meaning mapping values to [0,1] and sum(values)=1
      # dim=-1 is the last dimension, dim 1 here
      # meaning softmax is applied to every row
      dist = F.softmax(out, dim=-1)      
      # multinomial is extended binomial distribution
      # so like throwing a biased dice with 64 sides and using the probabilities 
      # from a row
      # it returns the index 
      # but in our case this is the same as the encoding of a token
      next = torch.multinomial(dist, num_samples=1)
      x = torch.cat((x,next), dim=1)
    return x

# the transformer model
class NanoShakesModel(nn.Module):
  def __init__(self, embd_size, head_size):
    super().__init__()
    self.multiHead = MultiHeadAttentionBlock(embd_size, head_size)
    self.endFF = FFEndBlock(input_size)
    self.norm1 = nn.LayerNorm()
    self.norm2 = nn.LayerNorm()

  def forward(self, x):
    # TODO: don't know why this is additive
    x = x + self.multiHead(self.norm1(x))
    x = x + self.endFF(self.norm2(x))
    return x

class MultiHeadAttentionBlock(nn.Module):
  def __init__(self, embd_size, head_size):
    super().__init__()
    head_size = embd_size
    self.head = Head(embd_size, head_size)

  def forward(self, x):
    return self.head(x)

class Head(nn.Module):
  def __init__(self, embd_size, head_size):
    super().__init__()
    self.q = nn.Linear(embd_size, head_size, bias = False)
    self.k = nn.Linear(embd_size, head_size, bias = False)
    self.v = nn.Linear(embd_size, head_size, bias = False)

  def forward(self, x):
    return (self.q(x) @ self.k(x).transpose()) @ self.v(x)

class FFEndBlock(nn.Module):
  def __init__(self, embd_size):
    super().__init__()
    self.net = nn.Sequential(
        nn.Linear(embd_size, 4*embd_size),
        nn.ReLU(),
        nn.Linear(4*embd_size, embd_size),
        # TODO: check what this actually does
        nn.Dropout(dropout))

  def forward(self, x):
    return self.net(x)
# tensor.item() returns the value as standard python type
# obj2 = obj1.to(device) if obj1 is a model it's moved to the device and obj1 references the same as obj2, but if it's a tensor obj2 will be a copy of obj1
# bias in a nn linear layer is a trainable additive constant on every node
# MyClass(InheritedClass) is the syntax for inheriting
# python classes have a virtual function named __call__ which is called when you use an instance variable like a function
# pytorch module class overrides __call__ to call the module.forward function

# training

# better error estimation by averaging over differnt sets
@torch.no_grad()
def calcAverageError(model):  
  model.eval()
  losses = {key:0.0 for key in ['train', 'val']}
  for key in losses:    
    for x in range(eval_average_loss_n):
      xb, yb = getBatch(key)
      _, loss = model(xb, yb)
      losses[key] += loss.item()
  losses = {key:val/eval_average_loss_n for key, val in losses.items()}
  model.train()
  return losses

print("Using " + device)
model = SimpleModel(vocab_size)
model.to(device)
print(sum(p.numel() for p in model.parameters())/1e6, 'M parameters')
opt = torch.optim.AdamW(model.parameters(), lr = learning_rate)

for epoch in range(epoch_count):
  xBatch, yBatch = getBatch('train')

  for i in range(len(xBatch)):
    # forward
    output, loss = model(xBatch, yBatch)
    # backwards
    # resets the gradients, e.g. in RNNs you wouldn't necessarily do this
    #loss = lossFct(output, y[i])
    opt.zero_grad(set_to_none=True) 
    loss.backward()
    opt.step()
  
  if epoch%eval_interval == 0 or epoch==epoch_count-1:
    losses = calcAverageError(model)
    print(f"Epoch: {epoch}\t\tTrainloss: {losses['train']:.4f}\t\tValloss: {losses['val']:.4f}")

print(loss.item())

# just a tensor to start with and configuring the device its on
start = torch.zeros((1, 1), dtype=torch.long, device=device)
print(decode(model.generate(start, 500)[0].tolist()))